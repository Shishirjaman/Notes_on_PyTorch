{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **AutoGrad**"
      ],
      "metadata": {
        "id": "pDTVaEHhF_yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Autograd** (automatic differentiation) is PyTorch’s engine that automatically computes gradients (partial derivatives) for tensor computations, by recording operations on tensors in a dynamic computation graph and then running backprop when you call .backward().\n",
        "​\n",
        "\n",
        "**Why autograd is required:**\n",
        "Training neural networks uses gradient-based optimization (e.g., gradient descent), which needs gradients like ∂loss/∂θ for millions of parameters. Autograd removes the need to manually derive and code these derivatives, which becomes impractical for deep/complex networks.\n",
        "​\n",
        "\n",
        "**Key points to remember**\n",
        "\n",
        "\n",
        "*   Only tensors with requires_grad=True are tracked for gradients, and during .backward() gradients are accumulated into .grad only for leaf tensors with requires_grad=True.\n",
        "*   Gradients accumulate by default across multiple .backward() calls, so you usually clear them each iteration (optimizer.zero_grad() or param.grad = None).\n",
        "\n",
        "\n",
        "*   Use torch.no_grad() (or detach()) during inference to avoid tracking and save memory/compute."
      ],
      "metadata": {
        "id": "TrAsMqzDdsvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1:** y = x^2"
      ],
      "metadata": {
        "id": "hABy9G6QGKnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qu_2u5AeF9sC"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0, requires_grad=True) # requires_grad by default is False and setting it True means we need gradient value of \"x\""
      ],
      "metadata": {
        "id": "3k_qT8sDGNzE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XulvKP_GYCP",
        "outputId": "979729ab-bbc6-46f0-e23f-35ff117bb4c4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "MPLi9S_LGaJ2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PfNG4QgGeRV",
        "outputId": "e52dd77e-9903-428b-bb06-ac89aba72019"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(9., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward() # it will does backward propagation with respect to x (because we set requires_grad true in x)"
      ],
      "metadata": {
        "id": "tEmAMA_IGfQx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad # it will return the gradient value of x after brackward propagation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQBMwnztGhxp",
        "outputId": "97f34ca0-ac82-4103-97ad-47c625fc1ed7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2: ** z = sin(x^2)"
      ],
      "metadata": {
        "id": "TCcG9wIVG5_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(4.0, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nGnJ79TGmZU",
        "outputId": "ca060872-a872-4935-80e1-1152d93e34c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x **2\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndqrxPi3K-7I",
        "outputId": "4544da14-4f96-4113-fc19-74ae57df156c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.sin(y)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOkRuPj3LCGS",
        "outputId": "ee5eaeff-0384-4153-a6dc-2b49d50143d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.2879, grad_fn=<SinBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "mOXzgjJQLIPY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuatofHuLKbY",
        "outputId": "042599f3-4908-417e-a3a6-ae67913dfe02"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-7.6613)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3:** calculating gradient of w and b of a simple nn"
      ],
      "metadata": {
        "id": "C9OAHTjfOFla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(6.7)\n",
        "y = torch.tensor(0.0)"
      ],
      "metadata": {
        "id": "CDyF4zytLL5e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "BFwH2IlYOiOP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkMSuIorOw8x",
        "outputId": "1bb59deb-d09c-4c10-aaf7-9c53ff394bce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1., requires_grad=True)\n",
            "tensor(0., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z = w * x + b\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkK88k51PAQ4",
        "outputId": "d3f32256-7186-43c7-d008-7009cd59a335"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7000, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = torch.sigmoid(z)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqpZ80JQPKc-",
        "outputId": "6bd9be0a-bdae-43d4-fe3a-0e2f56e2a663"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9988, grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy_loss(predicted, target):\n",
        "  epsilon = 1e-8 # to prevent log(0)\n",
        "  predicted = torch.clamp(predicted, epsilon, 1-epsilon)\n",
        "\n",
        "  return -(target * torch.log(predicted) + (1-target) * torch.log(1-predicted))"
      ],
      "metadata": {
        "id": "bxi9W2UePVsi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = binary_cross_entropy_loss(y_pred, y)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mk6EpWHGQxQN",
        "outputId": "07e57325-b770-4019-db00-0f579c827d10"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.7012, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "id": "aD4QWDLqQ9iC"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmVxEDntRD_O",
        "outputId": "24893c68-8a4f-40f1-b25d-4d068d96d10b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6918)\n",
            "tensor(0.9988)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Example 4:** y = x^2 where x is vector"
      ],
      "metadata": {
        "id": "Fxm9Y9bsSbvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py72_SsQRH0E",
        "outputId": "7f857142-8934-460c-f904-959c3f09bea1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = (x ** 2).mean()\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ya4zZ25SwmX",
        "outputId": "c1facb97-52a0-4f38-a3cc-693a80d0ba23"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.6667, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "iPY3dDrNSzoS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmKtEkEWTCuc",
        "outputId": "60476f11-1aa6-41e9-832a-70375a1124fd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6667, 1.3333, 2.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clearning Grad**\n",
        "We need to clear grad after calculation otheriwse gradient will accumulate with the previous gradient value.\n",
        "Means: Suppord I got,\n",
        " **x_grad = 4** after calculating forward prop of y = x^2\n",
        "Now if I don't clear the grad and run the forward prop again at the end\n",
        "**x_grad** will become **8** by adding x_grad with the previous x_grad **(4 + 4) = 8**. And it will keep adding 4 with the previous grad after each iteration. That is why we need to clear grad."
      ],
      "metadata": {
        "id": "JWGydONxToPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clearning grad\n",
        "x = torch.tensor(4.0, requires_grad=True)"
      ],
      "metadata": {
        "id": "3l8ATuUnTFnm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2"
      ],
      "metadata": {
        "id": "crhYwSJdXBnC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "k9SA6bb7XFUe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-XwTbS2XHz1",
        "outputId": "a358cc86-6eb5-45da-e6b2-97d307121088"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(8.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad.zero_()) # it will clear the grad or I can say set the grad 0 before new iteration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPP6AbAmXKdy",
        "outputId": "670c00d6-0ed6-4fb0-b184-9fd57f95d3da"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Disable Gradient Tracking**"
      ],
      "metadata": {
        "id": "8Nb6NpHlX5f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_b9Y8ioXdqt",
        "outputId": "98b2d8de-aeb9-461f-afa2-c42b1184a7b9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSDCeGQkYfwY",
        "outputId": "4fb480f5-e3ec-4227-acf9-9283471ccfcf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4., grad_fn=<PowBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()"
      ],
      "metadata": {
        "id": "iHyH3C8SYomZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD_glz7XYref",
        "outputId": "79aafb02-e1f1-4044-e4b9-1245187bea21"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 1: requires_grad_(False)\n",
        "\n",
        "x.requires_grad_(False)\n",
        "print(x) # removes requires_grad true means it will not keep track of the gradient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgteWUjWYw2x",
        "outputId": "4db28451-a51f-497e-84e8-d0b654dd7579"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = x ** 2\n",
        "print(y) # y also removed backward tracking, so if I run y.backward() after that I will get error for no grad track."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9UKk-XmZZkM",
        "outputId": "3884c404-50fc-42bb-ed10-d45b66b24ecb"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 2: detach()\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "z = x.detach() #This will create a copy of x and assign to z without grad tracking.\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6ECqiWNZnSB",
        "outputId": "2fd850d5-0e6a-4f7a-883c-e81ccc0d1cfa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n",
            "tensor(2.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = z ** 2\n",
        "print(y) # will get the same y value without backprop tracking for grad."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuWEn1RlaZiU",
        "outputId": "d5e42b64-f69f-4af2-8f7e-9dbf7a86c7f8"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# option 3: torch.no_grad()\n",
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpQtL6m-azta",
        "outputId": "3aeb8ada-3aff-4c7e-b7d3-d0ccadaac045"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y = x ** 2 # it will not track y for backprop. so y.backward() will not works for  this y, ultimatly we can Disable grad of x here.\n",
        "\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUQUwwOsbLMh",
        "outputId": "a8171434-a07d-4942-9c84-bb0756a1aad1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n"
          ]
        }
      ]
    }
  ]
}